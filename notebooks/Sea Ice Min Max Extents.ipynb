{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Compute summary statistics for the daily sea ice index.\n",
    "\n",
    "# From the CSV files determine the day of maximum and minimum extent for each\n",
    "# month and how that month's max and min ranks with all other months\n",
    "\n",
    "The input data format is just a date and extent for each day we have data.\n",
    "```\n",
    "Year, Month, Day,     Extent,    Missing, Source Data\n",
    "YYYY,    MM,  DD, 10^6 sq km, 10^6 sq km, Source data product web site: http://nsidc.org/d....\n",
    "1978,    10,  26,     10.231,      0.000, ftp://sidads.colorado.edu/pub/DATASETS/nsidc0051....\n",
    "1978,    10,  28,     10.420,      0.000, ftp://sidads.colorado.edu/pub/DATASETS/nsidc0051....\n",
    "1978,    10,  30,     10.557,      0.000, ftp://sidads.colorado.edu/pub/DATASETS/nsidc0051....\n",
    "....\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Start by downloading the daily sea ice extent data from NSIDC's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir -p ../data\n",
    "!wget -P ../data -qN ftp://sidads.colorado.edu/pub/DATASETS/NOAA/G02135/north/daily/data/NH_seaice_extent_final.csv \n",
    "!wget -P ../data -qN ftp://sidads.colorado.edu/pub/DATASETS/NOAA/G02135/north/daily/data/NH_seaice_extent_nrt.csv\n",
    "!wget -P ../data -qN ftp://sidads.colorado.edu/pub/DATASETS/NOAA/G02135/south/daily/data/SH_seaice_extent_final.csv\n",
    "!wget -P ../data -qN ftp://sidads.colorado.edu/pub/DATASETS/NOAA/G02135/south/daily/data/SH_seaice_extent_nrt.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "code to read the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_the_date(year, mm, dd):\n",
    "    return dt.date(int(year), int(mm), int(dd))\n",
    "\n",
    "def slurp_csv(filename):\n",
    "    data = pd.read_csv(filename, header = None, skiprows=2,\n",
    "                       names=[\"year\", \"mm\", \"dd\", \"extent\", \"missing\", \"source\"],\n",
    "                       parse_dates={'date':['year', 'mm', 'dd']},\n",
    "                       date_parser=parse_the_date, index_col='date')\n",
    "    data = data.drop('missing', axis=1)\n",
    "    return data\n",
    "\n",
    "def read_a_hemisphere(hemisphere):\n",
    "    the_dir = '../data'\n",
    "    final_prod_filename = os.path.join(the_dir, '{hemi}H_seaice_extent_final.csv'.format(hemi=hemisphere[0:1].upper()))\n",
    "    nrt_prod_filename = os.path.join(the_dir, '{hemi}H_seaice_extent_nrt.csv'.format(hemi=hemisphere[0:1].upper()))\n",
    "\n",
    "    final = slurp_csv(final_prod_filename)\n",
    "    nrt = slurp_csv(nrt_prod_filename)\n",
    "    all_data = pd.concat([final, nrt])\n",
    "    return all_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Read CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            extent                                             source\ndate                                                                 \n1978-10-26  17.634   ftp://sidads.colorado.edu/pub/DATASETS/nsidc0...\n1978-10-28  17.815   ftp://sidads.colorado.edu/pub/DATASETS/nsidc0...\n1978-10-30  17.671   ftp://sidads.colorado.edu/pub/DATASETS/nsidc0...\n1978-11-01  17.534   ftp://sidads.colorado.edu/pub/DATASETS/nsidc0...\n1978-11-03  17.493   ftp://sidads.colorado.edu/pub/DATASETS/nsidc0..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "north = read_a_hemisphere('north')\n",
    "south = read_a_hemisphere('south')\n",
    "south.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Add columns for year and month: We have do this because when we read the CSV file\n",
    "we converted the existing year/month/day columns into a python datetime object.\n",
    "also drop the source because we don't care where the data came from (near real time or production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_year_month_columns(df):\n",
    "    a = df.copy()\n",
    "    a = a.drop('source',1)\n",
    "    a = a.reset_index()\n",
    "    a['year'] = pd.to_datetime(a.date).dt.year\n",
    "    a['month'] = pd.to_datetime(a.date).dt.month\n",
    "    a = a.set_index('date')\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "north = add_year_month_columns(north)\n",
    "south = add_year_month_columns(south)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            extent  year  month\ndate                           \n1978-10-26  10.231  1978     10\n1978-10-28  10.420  1978     10\n1978-10-30  10.557  1978     10\n1978-11-01  10.670  1978     11\n1978-11-03  10.787  1978     11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "north.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            extent  year  month\ndate                           \n1978-10-26  17.634  1978     10\n1978-10-28  17.815  1978     10\n1978-10-30  17.671  1978     10\n1978-11-01  17.534  1978     11\n1978-11-03  17.493  1978     11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "south.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Add 5 day rolling mean to the timesereis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_rolling_mean(df, window=5, min_periods=2):\n",
    "    copy = df.copy()\n",
    "    # create an empty ts to align our extent data with\n",
    "    ts = pd.Series(np.nan, index=pd.date_range('1978-10-25', dt.date.today().strftime('%Y-%m-%d')))\n",
    "    copy.index = copy.index.to_datetime()\n",
    "    copy = df.align(ts, axis=0, join='right')[0]\n",
    "    df['5day-Avg'] = pd.rolling_mean(copy['extent'], window=5, min_periods=2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Want date back in the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "north = add_rolling_mean(north)\n",
    "south = add_rolling_mean(south)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            extent  year  month  5day-Avg\ndate                                     \n1978-10-26  10.231  1978     10       NaN"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "north.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         date  extent  year  month  5day-Avg\n0  1978-10-26  10.231  1978     10       NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "north = north.reset_index()\n",
    "south = south.reset_index()\n",
    "north.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use a groupby to compute the row locations that represent the minimum and\n",
    "maximum extent and grab those rows into new variables.  AKA: Filter out everything\n",
    "but the minimum/maximum extent for each month and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_min_and_max_variable_rows_by_year_and_month(df, variable):\n",
    "    min_groups = df.loc[df.groupby(['year','month'])[variable].idxmin()][['date', variable, 'year', 'month']]\n",
    "    max_groups = df.loc[df.groupby(['year','month'])[variable].idxmax()][['date', variable, 'year', 'month']]\n",
    "    return {'min': min_groups,  'max': max_groups}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "create dictionaries of max and min values for each hemisphere and for daily and rolling-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = select_min_and_max_variable_rows_by_year_and_month(north, 'extent')\n",
    "navg = select_min_and_max_variable_rows_by_year_and_month(north, '5day-Avg')\n",
    "s = select_min_and_max_variable_rows_by_year_and_month(south, 'extent')\n",
    "savg = select_min_and_max_variable_rows_by_year_and_month(south, '5day-Avg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "show that we have actually selected different data for daily and 5-average data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          date  extent  year  month\n48  1979-01-30  15.912  1979      1\n61  1979-02-25  16.579  1979      2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n['max'][3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          date   5day-Avg  year  month\n48  1979-01-30  15.795333  1979      1\n62  1979-02-27  16.515000  1979      2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "navg['max'][3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_rank(df, rank_by, ascending):\n",
    "    df['rank'] = df.groupby('month')[rank_by].rank(ascending=ascending)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "add rank column for each month and hemsiphere's max and min:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n['max'] = add_rank(n['max'], 'extent', ascending=False)\n",
    "n['min'] = add_rank(n['min'], 'extent', ascending=True)\n",
    "s['max'] = add_rank(s['max'], 'extent', ascending=False)\n",
    "s['min'] = add_rank(s['min'], 'extent', ascending=True)\n",
    "\n",
    "navg['max'] = add_rank(navg['max'], '5day-Avg', ascending=False)\n",
    "navg['min'] = add_rank(navg['min'], '5day-Avg', ascending=True)\n",
    "savg['max'] = add_rank(savg['max'], '5day-Avg', ascending=False)\n",
    "savg['min'] = add_rank(savg['min'], '5day-Avg', ascending=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_annual_min_max_ranking(df, field):\n",
    "    min_index = df.groupby(['year'])[field].idxmin()\n",
    "    max_index = df.groupby(['year'])[field].idxmax()\n",
    "    mindata = df.loc[min_index][['date', field]]\n",
    "    mindata['rank'] = mindata[field].rank(ascending=True)\n",
    "    maxdata = df.loc[max_index][['date', field]]\n",
    "    maxdata['rank'] = maxdata[field].rank(ascending=False)\n",
    "\n",
    "    mindata = mindata.set_index(pd.to_datetime(mindata.date).dt.year)\n",
    "    maxdata = maxdata.set_index(pd.to_datetime(maxdata.date).dt.year)\n",
    "\n",
    "    maxdata = maxdata.add_prefix('max_')\n",
    "    mindata = mindata.add_prefix('min_')\n",
    "\n",
    "    data = pd.concat([mindata, maxdata], axis=1)\n",
    "    return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is also desired that we have Annual min/max rank data so revisit the north and south"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "north_annual_by_day = do_annual_min_max_ranking(north, 'extent')\n",
    "north_annual_averaged = do_annual_min_max_ranking(north, '5day-Avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "south_annual_by_day = do_annual_min_max_ranking(south, 'extent')\n",
    "south_annual_averaged = do_annual_min_max_ranking(south, '5day-Avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        min_date  min_5day-Avg  min_rank    max_date  max_5day-Avg  max_rank\n1978  1978-12-31      7.596000        38  1978-10-28       17.7245        37\n1979  1979-02-19      2.928333        25  1979-09-15       18.3230        31\n1980  1980-02-26      2.574000         7  1980-09-25       19.0470        11"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "south_annual_averaged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Write out the data frames in a nice format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = navg['min'].copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'date', u'5day-Avg', u'year', u'month', u'rank'], dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             date                                                              \\\nyear         1978        1979        1980        1981        1982        1983   \nmonth                                                                           \n1             NaN  1979-01-02  1980-01-01  1981-01-01  1982-01-02  1983-01-01   \n2             NaN  1979-02-07  1980-02-02  1981-02-14  1982-02-01  1983-02-02   \n3             NaN  1979-03-31  1980-03-31  1981-03-30  1982-03-27  1983-03-30   \n4             NaN  1979-04-30  1980-04-30  1981-04-29  1982-04-30  1983-04-29   \n5             NaN  1979-05-30  1980-05-30  1981-05-31  1982-05-30  1983-05-31   \n6             NaN  1979-06-29  1980-06-29  1981-06-30  1982-06-29  1983-06-30   \n7             NaN  1979-07-31  1980-07-31  1981-07-30  1982-07-31  1983-07-30   \n8             NaN  1979-08-30  1980-08-28  1981-08-31  1982-08-30  1983-08-31   \n9             NaN  1979-09-21  1980-09-05  1981-09-14  1982-09-13  1983-09-10   \n10     1978-10-28  1979-10-01  1980-10-01  1981-10-02  1982-10-01  1983-10-02   \n11     1978-11-01  1979-11-02  1980-11-02  1981-11-01  1982-11-02  1983-11-01   \n12     1978-12-03  1979-12-02  1980-12-02  1981-12-01  1982-12-02  1983-12-01   \n\n                                                      ...  rank            \\\nyear         1984        1985        1986        1987 ...  2006 2007 2008   \nmonth                                                 ...                   \n1      1984-01-02  1985-01-02  1986-01-01  1987-01-02 ...     7    6    8   \n2      1984-02-01  1985-02-03  1986-02-02  1987-02-01 ...     1    8   12   \n3      1984-03-30  1985-03-11  1986-03-30  1987-03-27 ...     1    3   14   \n4      1984-04-29  1985-04-30  1986-04-29  1987-04-30 ...     1    3   11   \n5      1984-05-31  1985-05-30  1986-05-31  1987-05-30 ...     1    7    8   \n6      1984-06-30  1985-06-29  1986-06-30  1987-06-29 ...     5    6    9   \n7      1984-07-30  1985-07-31  1986-07-30  1987-07-31 ...    10    2    9   \n8      1984-08-31  1985-08-30  1986-08-31  1987-08-31 ...    11    2    4   \n9      1984-09-16  1985-09-11  1986-09-10  1987-09-05 ...    12    2    4   \n10     1984-10-02  1985-10-01  1986-10-02  1987-10-01 ...    10    2    4   \n11     1984-11-01  1985-11-02  1986-11-01  1987-11-01 ...     7    2   13   \n12     1984-12-01  1985-12-02  1986-12-01  1987-12-01 ...     1    4   11   \n\n                                          \nyear  2009 2010 2011 2012 2013 2014 2015  \nmonth                                     \n1        4   10    1   11    3    2    5  \n2       11    5    2    3    9    7    6  \n3        9    7    5   12   10    6    2  \n4       18   19    7   16    8    6    4  \n5       12    2    3   11   13    5   31  \n6       11    1    3    2    7    4  NaN  \n7        6    4    3    1    5    7  NaN  \n8        7    5    3    1    6    8  NaN  \n9        8    5    3    1    7    6  NaN  \n10       7    5    3    1    8    6  NaN  \n11       5    3    4    1    6    8  NaN  \n12       6    2    5    3    8    7  NaN  \n\n[12 rows x 114 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.set_index(['year', 'month']).unstack('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "month_names = [calendar.month_name[x] for x in range(1,13)]\n",
    "\n",
    "def swap_column_level_and_sort(df):\n",
    "    df.columns = df.columns.swaplevel(1,0)\n",
    "    df = df.sortlevel(0, axis=1)\n",
    "    return df\n",
    "\n",
    "# set index to year and month and then broadcast month back across the columns.\n",
    "# next swap and sort so that you have the data grouped under the month.\n",
    "def prepare_for_csv(df):\n",
    "    df = df.set_index(['year','month']).unstack('month')\n",
    "    df = swap_column_level_and_sort(df)\n",
    "    df.columns = df.columns.set_levels(month_names, level=0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_to_xls(df_list, writer, is_monthly=True):\n",
    "    for df, sheet in df_list:\n",
    "        if is_monthly:\n",
    "            df = prepare_for_csv(df)\n",
    "        df.to_excel(writer,'{sheet}'.format(sheet=sheet), float_format=\"%.3f\")\n",
    "\n",
    "\n",
    "writer = ExcelWriter('../output/Sea_Ice_MinMax_Statistics.xls')\n",
    "\n",
    "monthly_dataframelist =[(navg['min'], 'Northern 5day Min'),\n",
    "                        (navg['max'], 'Northern 5day Max'),\n",
    "                        (savg['min'], 'Southern 5day Min'),\n",
    "                        (savg['max'], 'Southern 5day Max'),\n",
    "                        (n['min'], 'Northern Daily Min'),\n",
    "                        (n['max'], 'Northern Daily Max'),\n",
    "                        (s['min'], 'Southern Daily Min'),\n",
    "                        (s['max'], 'Southern Daily Max')]\n",
    "\n",
    "annual_dataframelist = [(north_annual_averaged, 'North Annual 5day-avg'),\n",
    "                        (north_annual_by_day, 'North Annual Daily'),\n",
    "                        (south_annual_averaged, 'South Annual 5day-avg'),\n",
    "                        (south_annual_by_day, 'South Annual Daily')]\n",
    "\n",
    "write_to_xls(monthly_dataframelist, writer, is_monthly=True)\n",
    "write_to_xls(annual_dataframelist, writer, is_monthly=False)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "clean up your csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd ../data ; rm -f NH_seaice_extent_final.csv NH_seaice_extent_nrt.csv SH_seaice_extent_final.csv SH_seaice_extent_nrt.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  },
  "name": "Sea Ice Min Max Extents.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
